var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"See here for a list of references","category":"page"},{"location":"references/","page":"References","title":"References","text":"J. Mertz. Introduction to Optical Microscopy. 2 Edition (Cambridge University Press, 2019).\n\n\n\nL. Lucy. An iterative technique for the rectification of observed distributions. Astronomical Journal 79, 745 (1974).\n\n\n\nW. H. Richardson. Bayesian-Based Iterative Method of Image Restorationast. J. Opt. Soc. Am. 62, 55–59 (1972).\n\n\n\nP. J. Verveer and T. M. Jovin. Image restoration based on Good's roughness penalty with application to fluorescence microscopy. J. Opt. Soc. Am. A 15, 1077–1083 (1998).\n\n\n\nI. J. Good and R. A. Gaskins. Nonparametric Roughness Penalties for Probability Densities. Biometrika 58, 255–277 (1971).\n\n\n\n","category":"page"},{"location":"function_references/loss/#Loss-Functions","page":"Loss Functions","title":"Loss Functions","text":"","category":"section"},{"location":"function_references/loss/","page":"Loss Functions","title":"Loss Functions","text":"Poisson\npoisson_aux\nGauss\ngauss_aux\nScaledGauss\nscaled_gauss_aux","category":"page"},{"location":"function_references/loss/#DeconvOptim.Poisson","page":"Loss Functions","title":"DeconvOptim.Poisson","text":"Poisson()\n\nReturns a function to calculate Poisson loss Check the help of poisson_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.poisson_aux","page":"Loss Functions","title":"DeconvOptim.poisson_aux","text":"poisson_aux(μ, meas, storage=similar(μ))\n\nCalculates the Poisson loss for μ and meas. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.Gauss","page":"Loss Functions","title":"DeconvOptim.Gauss","text":"Gauss()\n\nReturns a function to calculate Gauss loss. Check the help of gauss_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.gauss_aux","page":"Loss Functions","title":"DeconvOptim.gauss_aux","text":"gauss_aux(μ, meas, storage=similar(μ))\n\nCalculates the Gauss loss for μ and meas. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.ScaledGauss","page":"Loss Functions","title":"DeconvOptim.ScaledGauss","text":"ScaledGauss()\n\nReturns a function to calculate scaled Gauss loss. Check the help of scaled_gauss_aux.\n\n\n\n\n\n","category":"function"},{"location":"function_references/loss/#DeconvOptim.scaled_gauss_aux","page":"Loss Functions","title":"DeconvOptim.scaled_gauss_aux","text":"scaled_gauss_aux(μ, meas, storage=similar(μ); read_var=0)\n\nCalculates the scaled Gauss loss for μ and meas. read_var=0 is the readout noise variance of the sensor. μ can be of larger size than meas. In that case we extract a centered region from μ of the same size as meas.\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#Regularizers","page":"Regularizers","title":"Regularizers","text":"","category":"section"},{"location":"function_references/regularizer/#CPU","page":"Regularizers","title":"CPU","text":"","category":"section"},{"location":"function_references/regularizer/","page":"Regularizers","title":"Regularizers","text":"TV\nTikhonov\nGR\nTH\nHS","category":"page"},{"location":"function_references/regularizer/#DeconvOptim.TV","page":"Regularizers","title":"DeconvOptim.TV","text":"TV(; <keyword arguments>)\n\nThis function returns a function to calculate the Total Variation regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: \nsum_dims=1:num_dims: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"forward\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"forward\".\nϵ=1f-8 is a smoothness variable, to make it differentiable\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> reg = TV(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"forward\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n12.649111f0\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.Tikhonov","page":"Regularizers","title":"DeconvOptim.Tikhonov","text":"Tikhonov(; <keyword arguments>)\n\nThis function returns a function to calculate the Tikhonov regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"laplace\": Either \"laplace\", \"spatial_grad_square\", \"identity\" accounting for different   modes of the Tikhonov regularizer. Default is \"laplace\".\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution.\n\njulia> reg = Tikhonov(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"identity\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n285\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.GR","page":"Regularizers","title":"DeconvOptim.GR","text":"GR(; <keyword arguments>)\n\nThis function returns a function to calculate the Good's roughness regularizer of a n-dimensional array. \n\nArguments\n\nnum_dims=2: Dimension of the array that should be regularized \nsum_dims=[1, 2]: A array containing the dimensions we want to sum over\nweights=nothing: A array containing weights to weight the contribution of    different dimensions. If weights=nothing all dimensions are weighted equally.\nstep=1: A integer indicating the step width for the array indexing\nmode=\"forward\": Either \"central\" or \"forward\" accounting for different   modes of the spatial gradient. Default is \"forward\".\nϵ=1f-8 is a smoothness variable, to make it differentiable\n\nExamples\n\nTo create a regularizer for a 3D dataset where the third dimension has different contribution. For the derivative we use forward mode.\n\njulia> reg = GR(num_dims=2, sum_dims=[1, 2], weights=[1, 1], mode=\"forward\");\n\njulia> reg([1 2 3; 4 5 6; 7 8 9])\n-26.36561871738898\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.TH","page":"Regularizers","title":"DeconvOptim.TH","text":"TH(; <keyword arguments>)\n\nThis function returns a function to calculate the Total Hessian norm of a n-dimensional array.\n\nArguments\n\nnum_dims=2\nϵ=1f-8 is a smoothness variable, to make it differentiable\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#DeconvOptim.HS","page":"Regularizers","title":"DeconvOptim.HS","text":"HS(; p=1)\n\nHessian Schatten norm. p determines which Schatten norm is used.\n\nThis regularizer only works with 2D arrays at the moment.\n\n\n\n\n\n","category":"function"},{"location":"function_references/regularizer/#CUDA","page":"Regularizers","title":"CUDA","text":"","category":"section"},{"location":"function_references/regularizer/","page":"Regularizers","title":"Regularizers","text":"TV_cuda","category":"page"},{"location":"function_references/regularizer/#DeconvOptim.TV_cuda","page":"Regularizers","title":"DeconvOptim.TV_cuda","text":"TV_cuda(; num_dims=2)\n\nThis function returns a function to calculate the Total Variation regularizer  of a 2 or 3 dimensional array. num_dims can be either 2 or 3.\n\njulia> using CUDA\n\njulia> reg = TV_cuda(num_dims=2);\n\njulia> reg(CuArray([1 2 3; 4 5 6; 7 8 9]))\n12.649111f0\n\n\n\n\n\n","category":"function"},{"location":"background/loss_functions/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Loss functions are generally introduced in mathematical optimization theory. The purpose is to map a certain optimization problem onto a real number. By minimizing this real number, one hopes that the obtained parameters provide a useful result for the problem.  One common loss function (especially in deep learning) is simply the L^2 norm between measurement and prediction.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"So far we provide three adapted loss functions with our package. However, it is relatively easy to incorporate custom defined loss functions or import them from packages like Flux.jl. The interface from Flux.jl is the same as for our loss functions.","category":"page"},{"location":"background/loss_functions/#Poisson-Loss","page":"Loss functions","title":"Poisson Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"As mentioned in Noise Model, Poisson shot noise is usually the dominant source of noise. Therefore one achieves good results by choosing a loss function which considers both the difference between measurement and reconstruction but also the noise process. See [4] and [1] for more details on that. As key idea we interpret the measurement as a stochastic process. Our aim is to find a deconvolved image which describes as accurate as possible the measured image. Mathematically the probability for a certain measurement Y is","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r fracmu(r)^Y(r)Gamma(Y(r) + 1) exp(- mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where Y is the measurement, mu is the expected measurement (ideal measurement without noise) and Gamma is the generalized factorial function. In the deconvolution process we get Y as input and want to find the ideal specimen S which results in a measurement mu(r) = (S * textPSF)(r)). Since we want to find the best reconstruction, we want to find a mu(r) so that p(Y(r)  mu(r)) gets as large as possible. Because that means that we find the specimen which describes the measurement with the highest probability. Instead of maximizing p(Y(r)  mu(r)) a common trick is to minimize - log(p(Y(r)mu(r))).  Mathematically, the optimization of both functions provides same results but the latter is numerically more stable.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min (- log(p(Y(r)mu(r)))) = undersetS(r)arg min sum_r (mu(r) + log(Gamma(Y(r) + 1)) - Y(r) log(mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"which is equivalent to","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (mu(r)  - Y(r) log(mu(r))","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"since the second term only depends on Y(r) but not on mu(r). The gradient of L with respect to mu(r) is simply","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = 1 - fracY(r)mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The function L and the gradient nabla L are needed for any gradient descent optimization algorithm. The numerical evaluation of the Poisson loss can lead to issues. Since mu(r)=0 can happen for a measurement with zero intensity background. However, the loss is not defined for mu leq 0. In our source code we set all intensity values below a certain threshold epsilon to epsilon itself. This prevents the evaluation of the logarithm at undefined values.","category":"page"},{"location":"background/loss_functions/#Scaled-Gaussian-Loss","page":"Loss functions","title":"Scaled Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"It is well known that the Poisson density function behaves similar as a Gaussian density function for mugg 1. This approximation is almost for all use cases in microscopy valid since regions of interest in an image usually consists of multiple photons and not to a single measured photon. Mathematically the Poisson probability can be approximately (using Stirling's formula in the derivation) expressed as:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) approx prod_r fracexp left(-frac(x-mu(r) )^22 mu(r) right)sqrt2 pi  mu(r) ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we get for the loss function:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r frac12 log(mu(r)) + frac(Y(r)-mu(r))^22 mu(r)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"The gradient is given by:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"nabla L = fracmu(r) + mu(r)^2 - Y(r)^22 mu^2","category":"page"},{"location":"background/loss_functions/#Gaussian-Loss","page":"Loss functions","title":"Gaussian Loss","text":"","category":"section"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"A very common loss in optimization (and Deep Learning) is a simple Gaussian loss. However, this loss is not recommended for low intensity microscopy since it doesn't consider Poisson noise. However, still combined with suitable regularizer reasonable results can be achieved. The probability is defined as ","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"p(Y(r)mu(r)) = prod_r frac1sqrt2 pi sigma^2 expleft(- frac(Y(r) - mu(r))^22 sigma ^2 right)","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"where sigma is the standard deviation of the Gaussian.","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Applying the negative logarithm we can simplify the loss to be minimized:","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"undersetS(r)arg min L = undersetS(r)arg min sum_r (Y(r) - mu(r))^2","category":"page"},{"location":"background/loss_functions/","page":"Loss functions","title":"Loss functions","text":"Since we are looking for mu(r) minimizing this expression, sigma is just a constant offset being irrelevant for the solution. This expression is also called L2 loss.","category":"page"},{"location":"function_references/mapping/#Mapping-Functions","page":"Mapping Functions","title":"Mapping Functions","text":"","category":"section"},{"location":"function_references/mapping/","page":"Mapping Functions","title":"Mapping Functions","text":"Non_negative\nMap_0_1\nPiecewise_positive\nPow4_positive\nAbs_positive","category":"page"},{"location":"function_references/mapping/#DeconvOptim.Non_negative","page":"Mapping Functions","title":"DeconvOptim.Non_negative","text":"Non_negative()\n\nReturns a function and an inverse function inverse function to map numbers to non-negative numbers. We use a parabola.\n\nExamples\n\njulia> p, p_inv = Non_negative()\n(DeconvOptim.var\"#5#7\"(), DeconvOptim.var\"#6#8\"())\n\njulia> x = [-1, 2, -3]\n3-element Array{Int64,1}:\n -1\n  2\n -3\n\njulia> p(x)\n3-element Array{Int64,1}:\n 1\n 4\n 9\n\njulia> p_inv(p(x))\n3-element Array{Float64,1}:\n 1.0\n 2.0\n 3.0\n\n\n\n\n\n","category":"function"},{"location":"function_references/mapping/#DeconvOptim.Map_0_1","page":"Mapping Functions","title":"DeconvOptim.Map_0_1","text":"Map_0_1()\n\nReturns a function and an inverse function to map numbers to an interval between 0 and 1. via an exponential function.\n\n\n\n\n\n","category":"function"},{"location":"function_references/mapping/#DeconvOptim.Piecewise_positive","page":"Mapping Functions","title":"DeconvOptim.Piecewise_positive","text":"Piecewise_positive()\n\nReturns a function and an inverse function  to map numbers to larger than 0 via two function stitched together.\n\n\n\n\n\n","category":"function"},{"location":"function_references/mapping/#DeconvOptim.Pow4_positive","page":"Mapping Functions","title":"DeconvOptim.Pow4_positive","text":"Pow4_positive()\n\nReturns a function and an inverse function to map numbers to larger than 0 with abs2.(abs2.(x))\n\n\n\n\n\n","category":"function"},{"location":"function_references/mapping/#DeconvOptim.Abs_positive","page":"Mapping Functions","title":"DeconvOptim.Abs_positive","text":"Abs_positive()\n\nReturns a function and an inverse function to map numbers to larger than 0. \n\n\n\n\n\n","category":"function"},{"location":"workflow/changing_regularizers/#Changing-Regularizers:-2D-Example","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"In this section we show how to change the regularizer and what are the different effects of it. The arguments of deconvolution we consider here are regularizer and lambda. regularizer specifies which regularizer is used. lambda specifies how strong the regularizer is weighted. The larger lambda the more you see the typical styles introduced by the regularizers.","category":"page"},{"location":"workflow/changing_regularizers/#Initializing","page":"Changing Regularizers: 2D Example","title":"Initializing","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Load the required modules for these examples:","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"using DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"As the next step we can prepare a noisy, blurred image.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"# load test images\nimg = convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\nh_view(img, img_b, img_n)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Good's-roughness-(GR)","page":"Changing Regularizers: 2D Example","title":"Let's test Good's roughness (GR)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"In this part we can look at the results produced with a GR regularizer. After inspecting the results, it becomes clear, that the benefit of 100 iterations is not really visible. In most cases approx 15 iterations produce good results. By executing GR() we in fact create a function which takes a array and returns a single value. ","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resGR100, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=100)\n@show optim_res\n\n@time resGR15, optim_res = deconvolution(img_n, psf, regularizer=GR(), iterations=15)\n@show optim_res\n\n@time resGR15_2, optim_res = deconvolution(img_n, psf, λ=0.05, regularizer=GR(), iterations=15)\n@show optim_res\n\nh_view(img_n, resGR100, resGR15, resGR15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Total-Variation-(TV)","page":"Changing Regularizers: 2D Example","title":"Let's test Total Variation (TV)","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"TV produces characteristic staircase artifacts. However, the results it produces are usually noise free and clear.","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resTV50, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=50)\n@show optim_res\n\n@time resTV15, optim_res = deconvolution(img_n, psf, regularizer=TV(), iterations=15)\n@show optim_res\n\n@time resTV15_2, optim_res = deconvolution(img_n, psf, λ=0.005, regularizer=TV(), iterations=15)\n@show optim_res\n\nh_view(img_n, resTV50, resTV15, resTV15_2)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-Tikhonov","page":"Changing Regularizers: 2D Example","title":"Let's test Tikhonov","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Tikhonov is not defined as precisely as the other two regularizers. Therefore we offer three different modes which differ quite a lot from each other. However, the results look all very similar","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@time resTik1, optim_res = deconvolution(img_n, psf, λ=0.001, regularizer=Tikhonov(), iterations=15)\n@show optim_res\n\n\n@time resTik2, optim_res = deconvolution(img_n, psf, λ=0.0001, \n                    regularizer=Tikhonov(mode=\"spatial_grad_square\"), iterations=15)\n@show optim_res\n\n@time resTik3, optim_res = deconvolution(img_n, psf, λ=0.0001, \n    regularizer=Tikhonov(mode=\"identity\"), iterations=15)\n@show optim_res\n\nh_view(img_n, resTik1, resTik2, resTik3)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"workflow/changing_regularizers/#Let's-test-without-regularizers","page":"Changing Regularizers: 2D Example","title":"Let's test without regularizers","text":"","category":"section"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"Usually optimizing without a regularizer is does not produce good results. The reason is, that the deconvolution tries to enhance high frequencies more and more with increasing iteration number.  However, high frequencies have low contrast and therefore the algorithm mostly enhances noise content (which is present in all frequency regions).","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"@jldoctest\n@time res100, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=50)\n@show optim_res\n\n@time res15, optim_res = deconvolution(img_n, psf, regularizer=nothing, iterations=15)\n@show optim_res\n\nh_view(img_n, 0.7 .* res100, res15)","category":"page"},{"location":"workflow/changing_regularizers/","page":"Changing Regularizers: 2D Example","title":"Changing Regularizers: 2D Example","text":"(Image: )","category":"page"},{"location":"function_references/analysis/#Analysis-functions","page":"Analysis functions","title":"Analysis functions","text":"","category":"section"},{"location":"function_references/analysis/#Quantitative-Criteria","page":"Analysis functions","title":"Quantitative Criteria","text":"","category":"section"},{"location":"function_references/analysis/","page":"Analysis functions","title":"Analysis functions","text":"DeconvOptim.relative_energy_regain\nDeconvOptim.normalized_cross_correlation","category":"page"},{"location":"function_references/analysis/#DeconvOptim.relative_energy_regain","page":"Analysis functions","title":"DeconvOptim.relative_energy_regain","text":"relative_energy_regain(ground_truth, rec)\n\nCalculates the relative energy regain between the ground_truth and the reconstruction. Assumes that both arrays are 2 dimensional\n\nReference\n\nRainer Heintzmann, \"Estimating missing information by maximum likelihood deconvolution\"\n\n\n\n\n\n","category":"function"},{"location":"function_references/analysis/#DeconvOptim.normalized_cross_correlation","page":"Analysis functions","title":"DeconvOptim.normalized_cross_correlation","text":"normalized_cross_correlation(ground_truth, measured)\n\nCalculates the normalized cross correlation.\n\nExternal links: \n\nWikipedia\nStatsBase.jl\n\n\n\n\n\n","category":"function"},{"location":"background/physical_background/#Physical-Background","page":"Physical Background","title":"Physical Background","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"We want to provide some physical background to the process of (de)convolution in optics. Optical systems like brightfield microscopes can only collect a certain amount of light emitted by a specimen. This effect (diffraction) leads to a blurred image of that specimen. Mathematically the lens has a certain frequency support. Within that frequency range, transmission of light is supported. Information (light) outside of this frequency support (equivalent to high frequency information) is lost. In the following picture we can see several curves in the frequency domain.  The orange line is a artificial object with a constant frequency spectrum (delta peak in real space). If such a delta peak is transferred through an optical lens, in real space the object is convolved with the point spread function (PSF).  In frequency space such a convolution is a multiplication of the OTF (OTF is the Fourier transform of the PSF) and the frequency spectrum of the object. The green dotted curve is the captured image after transmission through the system. Additionally some noise was introduced which can be recognized through some bumps outside of the OTF support. (Image: Frequency spectrum)","category":"page"},{"location":"background/physical_background/#Forward-Model","page":"Physical Background","title":"Forward Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Mathematically an ideal imaging process of specimen emitting incoherent light by a lens (or any optical system in general) can be described as:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where * being a convolution operation, r being the position, S being the sample and textPSF being the point spread function of the system. One can also introduce a background term b independent of the position, which models a constant signal offset of the imaging sensor:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + b","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"In frequency space (Fourier transforming the above equation) the equation with b=0 is:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"tilde Y(k) = (tilde S cdot tildetextPSF)(k)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where k is the spatial frequency and cdot represents term-wise multiplication (this is due to the convolution theorem of the Fourier transform). From that equation it is clear why the green and blue line in the plot look very similar. The reason is, that the orange line is constant and we basically multiply the OTF with the orange line. ","category":"page"},{"location":"background/physical_background/#Noise-Model","page":"Physical Background","title":"Noise Model","text":"","category":"section"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"However, the physical description (forward model) should also contain a noise term to reflect the measurement process in reality more accurately. ","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"Y(r) = (S * textPSF)(r) + N(r) = mu(r) + N(r)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where N being a noise term. In fluorescence microscopy the dominant noise is usually Poisson shot noise (see [1]). The origin of that noise is the quantum nature of photons. Since the measurement process spans over a time T only a discrete number of photons is detected (in real experiment the amount of photons per pixel is usually in the order of 10^1 - 10^3). Note that this noise is not introduced by the sensor and is just a effect due to quantum nature of light.  We can interpret every sensor pixel as a discrete random variable X. The expected value of that pixel would be mu(r) (true specimen convolved with the textPSF). Due to noise, the systems measures randomly a signal for X according to the Poisson distribution:","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"f(y mu) = fracmu^y exp(-mu)Gamma(y + 1)","category":"page"},{"location":"background/physical_background/","page":"Physical Background","title":"Physical Background","text":"where f is the probability density distribution, y the measured value of the sensor, mu the expected value and Gamma the generalized factorial function (Gamma function).","category":"page"},{"location":"workflow/3D_dataset/#D-Dataset","page":"3D Dataset","title":"3D Dataset","text":"","category":"section"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"We can also deconvolve a 3D dataset with a 3D PSF. The workflow, especially for the regularizers, must be adapted slightly for 3D.","category":"page"},{"location":"workflow/3D_dataset/#Code-Example","page":"3D Dataset","title":"Code Example","text":"","category":"section"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"First, load the 3D PSF and image.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"using Revise, DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\nimg = convert(Array{Float32}, channelview(load(\"obj.tif\")))\npsf = ifftshift(convert(Array{Float32}, channelview(load(\"psf.tif\"))))\npsf ./= sum(psf)\n# create a blurred, noisy version of that image\nimg_b = conv(img, psf, [1, 2, 3])\nimg_n = poisson(img_b, 300);","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"As the next step we need to create the regularizers. With num_dims we define how many dimensions our reconstruction image has.  With sum_dims we specify which dimensions of those should be included in the regularizing process.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"reg1 = TV(num_dims=3, sum_dims=[1, 2, 3])\nreg2 = Tikhonov(num_dims=3, sum_dims=[1, 2, 3], mode=\"identity\")","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"We can then invoke the deconvolution. For Tikhonov using identity mode a smaller lambda produces better results. In the first reconstruction we also specified the padding. This parameters adds some spacing around the reconstruction image to prevent wrap around effects of the FFT based deconvolution. However, since we don't have bright objects at the boundary of the image we don't see an impact of that parameter.","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"@time res, ores = deconvolution(img, psf, regularizer=reg1, loss=Poisson(),\n                          λ=0.05, padding=0.2, iterations=10);\n@time res2, ores = deconvolution(img, psf, regularizer=reg2, loss=Poisson(),\n                          λ=0.001, padding=0.0, iterations=10);","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"Finally we can inspect the results:","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"img_comb1 = [img[:, : ,32] res2[:, :, 32] res[:, :, 32] img_n[:, :, 32]]\nimg_comb2 = [img[:, : ,38] res2[:, :, 38] res[:, :, 38] img_n[:, :, 38]]\n\nimg_comb = cat(img_comb1, img_comb2, dims=1)\nimg_comb ./= maximum(img_comb)\n\nimshow([img[:, :, 20:end] res2[:, :, 20:end] res[:, :, 20:end] img_n[:, :, 20:end]])\ncolorview(Gray, img_comb)","category":"page"},{"location":"workflow/3D_dataset/","page":"3D Dataset","title":"3D Dataset","text":"(Image: )","category":"page"},{"location":"workflow/cuda/#CUDA","page":"CUDA","title":"CUDA","text":"","category":"section"},{"location":"workflow/cuda/","page":"CUDA","title":"CUDA","text":"We also support CUDA.jl.","category":"page"},{"location":"workflow/cuda/#Load","page":"CUDA","title":"Load","text":"","category":"section"},{"location":"workflow/cuda/","page":"CUDA","title":"CUDA","text":"Before using a CuArray simply invoke. ","category":"page"},{"location":"workflow/cuda/","page":"CUDA","title":"CUDA","text":"using CUDA","category":"page"},{"location":"workflow/cuda/","page":"CUDA","title":"CUDA","text":"Our routines need as input array either only Arrays or CuArrays. To get the deconvolution running, both the PSF and the measured  array needs to be a CuArray. See also our 3D example here.","category":"page"},{"location":"workflow/cuda/#Issues-with-Regularizers","page":"CUDA","title":"Issues with Regularizers","text":"","category":"section"},{"location":"workflow/cuda/","page":"CUDA","title":"CUDA","text":"However, our approach to express the regularizers with Tullio.jl is currently not performant with GPUs. Therefore, to use CuArrays with regularizers, you need to choose TV_cuda. Other regularizers are not yet supported since we hope that Tullio.jl will be one day mature enough to produce reasonable fast gradients for CUDA kernels as well.","category":"page"},{"location":"function_references/deconvolution/#Deconvolution","page":"Deconvolution","title":"Deconvolution","text":"","category":"section"},{"location":"function_references/deconvolution/","page":"Deconvolution","title":"Deconvolution","text":"deconvolution\nDeconvOptim.richardson_lucy_iterative","category":"page"},{"location":"function_references/deconvolution/#DeconvOptim.deconvolution","page":"Deconvolution","title":"DeconvOptim.deconvolution","text":"deconvolution(measured, psf; <keyword arguments>)\n\nComputes the deconvolution of measured and psf. Return parameter is a tuple with two elements. The first entry is the deconvolved image. The second return parameter  is the output of the optimization of Optim.jl\n\nMultiple keyword arguments can be specified for different loss functions, regularizers and mappings.\n\nArguments\n\nloss=Poisson(): the loss function taking a vector the same shape as measured. \nregularizer=nothing: A regularizer function, same form as loss.    See GR, TV, Tikhonov and the help page for different regularizers.\nλ=0.05: A float indicating the total weighting of the regularizer with    respect to the global loss function\nbackground=0: A float indicating a background intensity level.\nmapping=Non_negative(): Applies a mapping of the optimizer weight. Default is a              parabola which achieves a non-negativity constraint.\niterations=nothing: Specifies a number of iterations after the optimization.   definitely should stop. By default 20 iterations will be selected by generic_invert.jl,    if nothing is provided.\nconv_dims: A tuple indicating over which dimensions the convolution should happen.              per default conv_dims=1:ndims(psf)\nplan_fft=true: Boolean whether plan_fft is used. Gives a slight speed improvement.\npadding=0: an float indicating the amount (fraction of the size in that dimension)    of padded regions around the reconstruction. Prevents wrap around effects of the FFT.   A array with size(arr)=(400, 400) with padding=0.05 would result in reconstruction size of    (440, 440). However, if padding is >= 0.0, we only return the reconstruction cropped to the original size.   For negative paddings, the absolute value is used, but the result maintains the padded size.   padding=0 disables any padding.\nopt_package=Opt_Optim: decides which backend for the optimizer is used.\nopt=LBFGS(): The chosen optimizer which must fit to opt_package \nopt_options=nothing: Can be a options file required by Optim.jl. Will overwrite iterations.\ninitial=mean(measured): defines a value (or array) with the initial guess. This will be pulled through the inverse mapping function                    and extended with a mean value (if border regions are used).\ndebug_f=nothing: A debug function which must take a single argument, the current reconstruction.\n\nnote: Note\nIf you want to provide your PSF model, ensure that centered around the first entry of the array (psf[1]). You may need to use ifftshift for a PSF model or a measured PSF.\n\nExample\n\njulia> using DeconvOptim, TestImages, Colors, Noise;\n\njulia> img = Float32.(testimage(\"resolution_test_512\"));\n\njulia> psf = Float32.(generate_psf(size(img), 30));\n\njulia> img_b = conv(img, psf);\n\njulia> img_n = poisson(img_b, 300);\n\njulia> res, o = deconvolution(img_n, psf);\n\n\n\n\n\n","category":"function"},{"location":"function_references/deconvolution/#DeconvOptim.richardson_lucy_iterative","page":"Deconvolution","title":"DeconvOptim.richardson_lucy_iterative","text":"richardson_lucy_iterative(measured, psf; <keyword arguments>)\n\nClassical iterative Richardson-Lucy iteration scheme for deconvolution. measured is the measured array and psf the point spread function. Converges slower than the optimization approach of deconvolution\n\nKeyword Arguments\n\nregularizer=GR(): A regularizer function. Can be exchanged\nλ=0.05: A float indicating the total weighting of the regularizer with    respect to the global loss function\niterations=100: Specifies number of iterations.\nprogress: if not nothing, the progress will be monitored in a summary dictionary as obtained by             DeconvOptim.optionstracedeconv()\n\nExample\n\njulia> using DeconvOptim, TestImages, Colors, Noise;\n\njulia> img = Float32.(testimage(\"resolution_test_512\"));\n\njulia> psf = Float32.(generate_psf(size(img), 30));\n\njulia> img_b = conv(img, psf);\n\njulia> img_n = poisson(img_b, 300);\n\njulia> @time res = richardson_lucy_iterative(img_n, psf);\n\n\n\n\n\n","category":"function"},{"location":"function_references/deconvolution/#More-generic-alternative","page":"Deconvolution","title":"More generic alternative","text":"","category":"section"},{"location":"function_references/deconvolution/","page":"Deconvolution","title":"Deconvolution","text":"invert","category":"page"},{"location":"function_references/deconvolution/#DeconvOptim.invert","page":"Deconvolution","title":"DeconvOptim.invert","text":"invert(measured, rec0, forward; <keyword arguments>)\n\nTries to invert the forward model. forward is a function taking  an input with the shape of rec0 and returns an object which has the  same shape as measured\n\nMultiple keyword arguments can be specified for different loss functions, regularizers and mappings.\n\nArguments\n\nloss=Poisson(): the loss function being compatible to compare with measured.\nregularizer=nothing: A regularizer function, same form as loss.\nλ=0.05: A float indicating the total weighting of the regularizer with    respect to the global loss function\nmapping=Non_negative(): Applies a mapping of the optimizer weight. Default is a              parabola which achieves a non-negativity constraint.\niterations=nothing: Specifies a number of iterations after the optimization.   definitely should stop. Will be overwritten if opt_options is provided. Default: 20\nopt_package=Opt_Optim: decides which backend for the optimizer is used.\nopt=LBFGS(): The chosen optimizer which must fit to opt_package. \nopt_options=nothing: Can be a options file required by Optim.jl. Will overwrite iterations.\ndebug_f=nothing: A debug function which must take a single argument, the current reconstruction. \n\n\n\n\n\n","category":"function"},{"location":"workflow/flexible_invert/#More-complex-Invert","page":"More complex Invert","title":"More complex Invert","text":"","category":"section"},{"location":"workflow/flexible_invert/","page":"More complex Invert","title":"More complex Invert","text":"We also provide functionality to invert problems which are not a straightforward deconvolution like multi view deconvolution or a problem where several measurements with different properties and forward models are available.  The idea is that a forward model, a initial guess and the according measurements are in principle enough to invert the problem.","category":"page"},{"location":"workflow/flexible_invert/#Example","page":"More complex Invert","title":"Example","text":"","category":"section"},{"location":"workflow/flexible_invert/","page":"More complex Invert","title":"More complex Invert","text":"Look into the examples folder to see how it can work.","category":"page"},{"location":"workflow/changing_loss/#Changing-Loss-Function:-2D-Example","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"We can also change the loss function. However, the loss is the most important part guaranteeing good results. Therefore choosing different loss functions than the  provided ones, will most likely lead to worse results. We now compare all implemented loss functions of DeconvOptim.jl. However, we could also include loss functions of Flux.jl since they have the same interface as our loss functions.","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"Poisson() will most likely produce the best results in presence of Poisson Noise. For Gaussian Noise, Gauss() is a suitable option. ScaledGaussian() is an mathematical approximation of Poisson(). At the moment ScaledGaussian() is not recommended because of artifacts in certain images.","category":"page"},{"location":"workflow/changing_loss/#Code-Example","page":"Changing Loss Function: 2D Example","title":"Code Example","text":"","category":"section"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"This example is also hosted in a notebook on GitHub.","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"using Revise, DeconvOptim, TestImages, Images, FFTW, Noise, ImageView\n\n# custom image views\nimshow_m(args...) = imshow(cat(args..., dims=3))\nh_view(args...) = begin\n    img = cat(args..., dims=2)\n    img ./= maximum(img)\n    colorview(Gray, img)\nend\n\n# load test images\nimg = convert(Array{Float32}, channelview(testimage(\"resolution_test_512\")))\n\npsf = generate_psf(size(img), 30)\n\n# create a blurred, noisy version of that image\nimg_b = conv(img, psf, [1, 2])\nimg_n = poisson(img_b, 300);\n\n@time resP, optim_res = deconvolution(img_n, psf, loss=Poisson(), iterations=10)\n@show optim_res\n\n@time resG, optim_res = deconvolution(img_n, psf, loss=Gauss(), iterations=10)\n@show optim_res\n\n@time resSG, optim_res = deconvolution(img_n, psf, loss=ScaledGauss(), iterations=10)\n@show optim_res\n\nh_view(resP, resG, resSG)","category":"page"},{"location":"workflow/changing_loss/","page":"Changing Loss Function: 2D Example","title":"Changing Loss Function: 2D Example","text":"The left image is Poisson(), in the middle Gauss(). The right image is ScaledGauss(). (Image: )","category":"page"},{"location":"background/mathematical_optimization/#Mathematical-Optimization","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"Deconvolution was already described as an optimization problem in the 1970s by [2], [3]. Since then, many variants and different kinds of deconvolution algorithms were presented, but mainly based on the concept of Lucy-Richardson. We try to formulate convolution as an inverse physical problem and solve it using a convex optimization loss function so that we can use fast optimizers to find the optimum. The variables we want to optimize for, are the pixels of the reconstruction S(r). Therefore our reconstruction problem consists of several thousands to billion variables. Mathematically the optimization can be written as:","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersetS(r)arg min L(textFwd(S(r))) + textReg(S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"where textFwd represents the forward model (in our case convolution of S(r) with the textPSF), S(r) is ideal reconstruction, L the loss function and textReg is a regularizer. The regularizer  puts in some prior information about the structure of the object.  See the following sections for more details about each part.","category":"page"},{"location":"background/mathematical_optimization/#Map-Functions","page":"Mathematical Optimization","title":"Map Functions","text":"","category":"section"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"In some cases we want to restrict the optimizer to solutions with S(r) geq 0. Usually one uses boxed optimizer or penalties to prevent negativity. However, in some cases, a S(r)  0 can lead to issues during the optimization process. For that purpose we can introduce a mapping function. Instead of optimizing for S(r) we can optimize for some hat S(r) where M is the mapping function connection ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r)= M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"A simple mapping function leading to S(r) geq 0 is ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"M(hat S(r)) = hat S(r)^2","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"The optimization problem is then given by","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"undersethat S(r)arg min L(textFwd(M(hat S(r)))) + textReg(M(hat S(r)))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"After the optimization we need to apply M on hat S to get the reconstructed sample ","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"S(r) = M(hat S(r))","category":"page"},{"location":"background/mathematical_optimization/","page":"Mathematical Optimization","title":"Mathematical Optimization","text":"One could also choose different functions M to obtain reconstruction in certain intensity intervals.","category":"page"},{"location":"background/regularizer/#Regularizer","page":"Regularizer","title":"Regularizer","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Regularizer are commonly used in inverse problems and especially in deconvolution to obtain solutions which are optimal with respect to some prior.  So far we have included three common regularizer. The regularizer take the current reconstruction S(r) as argument and return a scalar value. This value should be also minimized and is also added to the loss function. Each regularizer produces some characteristic image styles.","category":"page"},{"location":"background/regularizer/#Good's-Roughness-(GR)","page":"Regularizer","title":"Good's Roughness (GR)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Good's roughness definition was taken from [5] and [4]. For Good's roughness several identical expressions can be derived. We implemented the following one:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r sqrtS(r) (Delta_N sqrtS)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where N is the dimension of S(r). sqrt S is applied elementwise. Delta_n sqrtS(r) is the n-dimensional discrete Laplace operator. As 2D example where r = (xy):","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"(Delta_n sqrtS)(r) = fracsqrtS(x + s_x y) + sqrtS(x - s_x y) + sqrtS(x y+s_y) + sqrtS(x y-s_y) - 4 cdot sqrtS(x y)s_x cdot s_y","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where s_x and s_y are the stencil width in the respective dimension. The Laplace operator can be straightforwardly generalized to n dimensions. ","category":"page"},{"location":"background/regularizer/#Total-Variation-(TV)","page":"Regularizer","title":"Total Variation (TV)","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"As the name suggests, Total variation tries to penalize variation in the image intensity. Therefore it sums up the gradient strength at each point of the image. In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_r  (nabla S)(r)","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"Since we look at the magnitude of the gradient strength, this regularizer is anisotropic.","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"In 2D this is:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) = sum_xy sqrtS(x + 1 y) - S(x y)^2 + S(x y + 1) - S(x y)^2","category":"page"},{"location":"background/regularizer/#Tikhonov-Regularization","page":"Regularizer","title":"Tikhonov Regularization","text":"","category":"section"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"The Tikhonov regularizer is not as specific defined as Good's Roughness or Total Variation. In general Tikhonov regularization is defined by:","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"textReg(S(r)) =  (Gamma S)(r) _2^2","category":"page"},{"location":"background/regularizer/","page":"Regularizer","title":"Regularizer","text":"where Gamma is an operator which can be chosen freely. Common options are the identity operator which penalizes therefore just high intensity values. Another option would be the spatial gradient which would result in a similar operator to TV. And the last option we implemented is the spatial Laplace.","category":"page"},{"location":"workflow/performance_tips/#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"","category":"section"},{"location":"workflow/performance_tips/#Regularizer","page":"Performance Tips","title":"Regularizer","text":"","category":"section"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"The regularizers are built during calling with metaprogramming. Every time you call TV() it creates a new version which is evaluated with eval. In the first deconvolution routine it has to compile this piece of code. To prevent the compilation every time, define ","category":"page"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"reg = TV()","category":"page"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"which is then later used as a variable. In a notebook or REPL environment just define it in a different cell.","category":"page"},{"location":"workflow/performance_tips/#No-Regularizer","page":"Performance Tips","title":"No Regularizer","text":"","category":"section"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"Often the results are good without regularizer but then need to be early stopped (e.g. like iterations=20). This increases the performance drastically, but might lead to more artifacts in certain regions.","category":"page"},{"location":"workflow/performance_tips/#Optimizer","page":"Performance Tips","title":"Optimizer","text":"","category":"section"},{"location":"workflow/performance_tips/#L-BFGS","page":"Performance Tips","title":"L-BFGS","text":"","category":"section"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"You can also try to adjust the settings of the L-BFGS algorithm of Optim.jl Try to change m in opt=LBFGS(linesearch=BackTracking(), m=10). m is the history value of the L-BFGS algorithm. Smaller is usually faster, but might lead to worse results.  See also Wikipedia.","category":"page"},{"location":"workflow/performance_tips/#Line-Search","page":"Performance Tips","title":"Line Search","text":"","category":"section"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"L-BFGS uses LineSearches.jl. In our examples BackTracking turned out to be the fastest, but it might be worth to try different ones.","category":"page"},{"location":"workflow/performance_tips/#Iterations","page":"Performance Tips","title":"Iterations","text":"","category":"section"},{"location":"workflow/performance_tips/","page":"Performance Tips","title":"Performance Tips","text":"Try to set the keyword iterations=20 to a lower number if you want to early stop the deconvolution. Of course, the results might be worse then.","category":"page"},{"location":"function_references/utils/#Util-functions","page":"Util functions","title":"Util functions","text":"","category":"section"},{"location":"function_references/utils/#Convolution-Functions","page":"Util functions","title":"Convolution Functions","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"conv\nconv_psf\nplan_conv\nplan_conv_psf\nDeconvOptim.next_fast_fft_size","category":"page"},{"location":"function_references/utils/#DeconvOptim.conv","page":"Util functions","title":"DeconvOptim.conv","text":"conv(u, v[, dims])\n\nConvolve u with v over dims dimensions with an FFT based method. Note, that this method introduces wrap-around artifacts without proper padding/windowing.\n\nArguments\n\nu is an array in real space.\nv is the array to be convolved in real space as well.\nPer default ntuple(+, min(N, M))) means that we perform the convolution    over all dimensions of that array which has less dimensions.    If dims is an array with integers, we perform convolution    only over these dimensions. Eg. dims=[1,3] would perform the convolution   over the first and third dimension. Second dimension is not convolved.\n\nIf u and v are both a real valued array we use rfft and hence the output is real as well. If either u or v is complex we use fft and output is hence complex.\n\nExamples\n\n1D with FFT over all dimensions. We choose v to be a delta peak. Therefore convolution should act as identity.\n\njulia> u = [1 2 3 4 5]\n1×5 Array{Int64,2}:\n 1  2  3  4  5\njulia> v = [0 0 1 0 0]\n1×5 Array{Int64,2}:\n 0  0  1  0  0\njulia> conv(u, v)\n1×5 Matrix{Float64}:\n 4.0  5.0  1.0  2.0  3.0\n\n2D with FFT with different dims arguments.\n\njulia> u = 1im .* [1 2 3; 4 5 6]\n2×3 Matrix{Complex{Int64}}:\n 0+1im  0+2im  0+3im\n 0+4im  0+5im  0+6im\njulia> v = [1im 0 0; 1im 0 0]\n2×3 Matrix{Complex{Int64}}:\n 0+1im  0+0im  0+0im\n 0+1im  0+0im  0+0im\njulia> conv(u, v)\n2×3 Matrix{ComplexF64}:\n -5.0+0.0im  -7.0+0.0im  -9.0+0.0im\n -5.0+0.0im  -7.0+0.0im  -9.0+0.0im\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.conv_psf","page":"Util functions","title":"DeconvOptim.conv_psf","text":"conv_psf(u, psf[, dims])\n\nconv_psf is a shorthand for conv(u,ifftshift(psf)). For examples see conv.\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.plan_conv","page":"Util functions","title":"DeconvOptim.plan_conv","text":"plan_conv(u, v [, dims])\n\nPre-plan an optimized convolution for arrays shaped like u and v (based on pre-plan FFT) along the given dimensions dims. dims = 1:ndims(u) per default. The 0 frequency of u must be located at the first entry. We return two arguments:  The first one is v_ft (obtained by fft(v) or rfft(v)). The second return is the convolution function pconv. pconv itself has two arguments. pconv(u, v_ft=v_ft) where u is the object and v_ft the v_ft. This function achieves faster convolution than conv(u, u). Depending whether u is real or complex we do ffts or rffts\n\nWarning\n\nThe resulting output of the pconv function is a reference to an internal, allocated array. If you use the pconv function for different tasks,  a new call to pconv will change the previous result (since the previous result was only a reference, not a new array). \n\nExamples\n\njulia> u = [1 2 3 4 5]\n1×5 Matrix{Int64}:\n 1  2  3  4  5\njulia> v = [1 0 0 0 0]\n1×5 Matrix{Int64}:\n 1  0  0  0  0\njulia> v_ft, pconv = plan_conv(u, v);\njulia> pconv(u, v_ft)\n1×5 Matrix{Float64}:\n 1.0  2.0  3.0  4.0  5.0\njulia> pconv(u)\n1×5 Matrix{Float64}:\n 1.0  2.0  3.0  4.0  5.0\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.plan_conv_psf","page":"Util functions","title":"DeconvOptim.plan_conv_psf","text":"plan_conv_psf(u, psf [, dims]) where {T, N}\n\nplan_conv_psf is a shorthand for plan_conv(u, ifftshift(psf)). For examples see plan_conv.\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.next_fast_fft_size","page":"Util functions","title":"DeconvOptim.next_fast_fft_size","text":"next_fast_fft_size(x)\n\nx is a tuple of sizes. It rounds to the next fast FFT size. FFT is especially fast on small prime factors.\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Point-Spread-Function","page":"Util functions","title":"Point Spread Function","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_psf","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_psf","page":"Util functions","title":"DeconvOptim.generate_psf","text":"generate_psf(psf_size, radius)\n\nGeneration of an approximate 2D PSF. psf_size is the output size of the PSF. The PSF will be centered around the point [1, 1], radius indicates the pupil diameter in pixel from which the PSF is generated.\n\nnote: Note\nReturned 2D PSF is fftshifted in contrast to models, you can find in  literature.\n\nExamples\n\njulia> generate_psf([5, 5], 2)\n5×5 Array{Float64,2}:\n 0.36       0.104721    0.0152786    0.0152786    0.104721\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.0152786  0.00444444  0.000648436  0.000648436  0.00444444\n 0.104721   0.0304627   0.00444444   0.00444444   0.0304627\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Interpolation-and-downsampling","page":"Util functions","title":"Interpolation and downsampling","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"generate_downsample\nmy_interpolate","category":"page"},{"location":"function_references/utils/#DeconvOptim.generate_downsample","page":"Util functions","title":"DeconvOptim.generate_downsample","text":"generate_downsample(num_dim, downsample_dims, factor)\n\nGenerate a function (based on Tullio.jl) which can be used to downsample arrays. num_dim (Integer) are the dimensions of the array. downsample_dims is a list of which dimensions should be downsampled. factor is a downsampling factor. It needs to be an integer number.\n\nExamples\n\njulia> ds = generate_downsample(2, [1, 2], 2) \n[...]\njulia> ds([1 2; 3 4; 5 6; 7 8])\n2×1 Array{Float64,2}:\n 2.5\n 6.5\n\njulia> ds = generate_downsample(2, [1], 2)\n[...]\njulia> ds([1 2; 3 5; 5 6; 7 8])\n2×2 Array{Float64,2}:\n 2.0  3.5\n 6.0  7.0\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.my_interpolate","page":"Util functions","title":"DeconvOptim.my_interpolate","text":"my_interpolate(arr, size_n, [interp_type])\n\nInterpolates arr to the sizes provided in size_n. Therefore it holds ndims(arr) == length(size_n). interp_type specifies the interpolation type. See Interpolations.jl for all options\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#Center-Methods","page":"Util functions","title":"Center Methods","text":"","category":"section"},{"location":"function_references/utils/","page":"Util functions","title":"Util functions","text":"center_extract\ncenter_set!\ncenter_pos","category":"page"},{"location":"function_references/utils/#DeconvOptim.center_extract","page":"Util functions","title":"DeconvOptim.center_extract","text":"center_extract(arr, new_size_array)\n\nExtracts a center of an array.  new_size_array must be list of sizes indicating the output size of each dimension. Centered means that a center frequency stays at the center position. Works for even and uneven. If length(new_size_array) < length(ndims(arr)) the remaining dimensions are untouched and copied.\n\nExamples\n\njulia> DeconvOptim.center_extract([1 2; 3 4], [1]) \n1×2 Array{Int64,2}:\n 3  4\njulia> DeconvOptim.center_extract([1 2; 3 4], [1, 1])\n1×1 Array{Int64,2}:\n 4\njulia> DeconvOptim.center_extract([1 2 3; 3 4 5; 6 7 8], [2 2])\n2×2 Array{Int64,2}:\n 1  2\n 3  4\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_set!","page":"Util functions","title":"DeconvOptim.center_set!","text":"center_set!(arr_large, arr_small)\n\nPuts the arr_small central into arr_large. The convention, where the center is, is the same as the definition as for FFT based centered. Function works both for even and uneven arrays.\n\nExamples\n\njulia> DeconvOptim.center_set!([1, 1, 1, 1, 1, 1], [5, 5, 5])\n6-element Array{Int64,1}:\n 1\n 1\n 5\n 5\n 5\n 1\n\n\n\n\n\n","category":"function"},{"location":"function_references/utils/#DeconvOptim.center_pos","page":"Util functions","title":"DeconvOptim.center_pos","text":"center_pos(x)\n\nCalculate the position of the center frequency. Size of the array is x\n\nExamples\n\njulia> DeconvOptim.center_pos(3)\n2\njulia> DeconvOptim.center_pos(4)\n3\n\n\n\n\n\n","category":"function"},{"location":"workflow/basic_workflow/#Basic-Workflow","page":"Basic Workflow","title":"Basic Workflow","text":"","category":"section"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"In this section we show the workflow for deconvolution of 2D and 3D images using different regularizers.  From these examples one can also understand the different effects of the regularizers.","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"The picture below shows the general principle of DeconvOptim.jl. Since we interpret deconvolution as an optimization we initialize the reconstruction variables rec.  rec is a array of pixels which are the variables we are optimizing for. Then we can apply some mapping eg. to reconstruct only pixels having non-negative intensity values. Afterwards we compose the total loss functions. It consists of a regularizing part (weighted with lambda) and a loss part. The latter one compares the current reconstruction with the measured image. Total loss adds both values to a single scalar value. Using Zygote.jl we calculate the gradient with respect to all pixel values of rec. Note, Zygote.jl calculates the gradient with a reverse mode. From performance point of view, that is necessary since the loss function is a mapping from many pixels to a single value (texttotal loss mathbbR^N mapsto mathbbR). We can plug this gradient and the loss function into Optim.jl. Optim.jl then minimizes this loss function. The different parts of the pipeline (mapping, forward, regularizer) can be exchanged and adapted to the users needs. In most cases changing the regularizer or the number of iterations is enough.","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"(Image: )","category":"page"},{"location":"workflow/basic_workflow/","page":"Basic Workflow","title":"Basic Workflow","text":"For all options, see the function references. Via the help of Julia (typing ? in the REPL) we can also access extensive help.","category":"page"},{"location":"#DeconvOptim.jl","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"A framework for deconvolution of images convolved with a Point Spread Function (PSF).","category":"page"},{"location":"#Overview","page":"DeconvOptim.jl","title":"Overview","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"In optics, especially in microscopy, measurements are done with lenses. These lenses support only certain frequencies and weaken the contrast of high frequency content. Furthermore, in many cases Poisson or Gaussian noise is introduced by the  quantum nature of light (Poisson shot noise) or sensors (readout noise). DeconvOptim.jl is a Julia solution to deconvolution reducing the blur of lenses and denoising the image. Our framework relies on several other tools: The deconvolution problem is stated as a convex optimization problem via a loss function. Hence we make use of Optim.jl and especially fast solvers like L-BFGS. Since such solvers require gradients (of the loss function) we use automatic differentiation (AD) offered by Zygote.jl for that. Of course, one could derive the gradient by hand, however that's error-prone and for some regularizers hard to do by hand. Furthermore, fast AD of the regularizers is hard to achieve if the gradients are written with for loops. Fortunately Tullio.jl provides an extensive and fast framework to get expressions which can derived by the AD in acceptable speed.","category":"page"},{"location":"#Installation","page":"DeconvOptim.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"To get the latest stable release of DeconvOptim.jl type ] in the Julia REPL:","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"] add DeconvOptim","category":"page"},{"location":"#Quick-Example","page":"DeconvOptim.jl","title":"Quick Example","text":"","category":"section"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Below is a quick example how to deconvolve a image which is blurred with a Gaussian Kernel.","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"using DeconvOptim, TestImages, Colors, ImageIO, Noise, ImageShow\n\n# load test image\nimg = Float32.(testimage(\"resolution_test_512\"))\n\n# generate simple Point Spread Function of aperture radius 30\npsf = Float32.(generate_psf(size(img), 30))\n\n# create a blurred, noisy version of that image\nimg_b = conv(img, psf)\nimg_n = poisson(img_b, 300)\n\n# deconvolve 2D with default options\n@time res, o = deconvolution(img_n, psf)\n\n# deconvolve 2D with no regularizer\n@time res_no_reg, o = deconvolution(img_n, psf, regularizer=nothing)\n\n# show final results next to original and blurred version\nGray.([img img_n res])","category":"page"},{"location":"","page":"DeconvOptim.jl","title":"DeconvOptim.jl","text":"Left image is the sample. In the middle we display the the noisy and blurred version captured with an optical system. The right image is the deconvolved image with default options. (Image: )","category":"page"}]
}
